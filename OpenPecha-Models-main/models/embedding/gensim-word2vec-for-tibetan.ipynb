{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from collections import defaultdict \n",
    "from pathlib import Path\n",
    "import logging\n",
    "import unicodedata\n",
    "\n",
    "import config\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"word2vec_classical_bo\"\n",
    "model_path = config.MODELS_DIR / model_name\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "corpus_path = config.DATA_DIR / \"classical_bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    for pecha_path in path.iterdir():\n",
    "        if not pecha_path.is_dir(): continue\n",
    "        for fn in pecha_path.iterdir():\n",
    "            if not 'tokenize' in fn.name: continue\n",
    "            yield fn\n",
    "            \n",
    "def is_punt(word):\n",
    "    for punt in [\"།\", \"།།\", \"༄༅\"]:\n",
    "        if punt in word:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def tokenize(text):\n",
    "    return [token for token in text.split() if token and not is_punt(token)]\n",
    "    \n",
    "def get_sentences(fns):\n",
    "    for fn in fns:\n",
    "        for sentence in fn.open('r'):\n",
    "            if len(sentence.split()) < 3: continue\n",
    "            yield tokenize(unicodedata.normalize(\"NFKC\", sentence.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(corpus_path)\n",
    "sentences = list(get_sentences(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_highest_freq_word(sentences, k=10):\n",
    "    word_freq = defaultdict(int)\n",
    "    for sent in sentences:\n",
    "        for i in sent:\n",
    "            word_freq[i] += 1\n",
    "    return sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['འི་', 'ར་', 'དང་', 'ས་', 'ལ་', 'ཀྱི་', 'དུ་', 'ལ', 'གྱི་', 'ནས་']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_highest_freq_word(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why I seperate the training of the model in 3 steps:\n",
    "I prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "1. `Word2Vec()`: \n",
    ">In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n",
    "2. `.build_vocab()`: \n",
    ">Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "3. `.train()`:\n",
    ">Finally, trains the model.<br>\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameters:\n",
    "\n",
    "* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "\n",
    "* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n",
    "\n",
    "\n",
    "* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "\n",
    "* `sample` <font color='purple'>=</font> <font color='green'>float</font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n",
    "\n",
    "\n",
    "* `alpha` <font color='purple'>=</font> <font color='green'>float</font> - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "\n",
    "* `min_alpha` <font color='purple'>=</font> <font color='green'>float</font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "\n",
    "* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension of word embedding\n",
    "The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition (Melamud et al., 2016) [3] or part-of-speech (POS) tagging (Plank et al., 2016) [4], while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis (Ruder et al., 2016) [5].\n",
    "\n",
    "- [3] -> http://arxiv.org/abs/1601.00893\n",
    "- [4] -> Plank, B., Søgaard, A., & Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. \n",
    "- [5] -> http://arxiv.org/abs/1609.02745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1105/626395223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m w2v_model = Word2Vec(min_count=20,\n\u001b[0m\u001b[1;32m      2\u001b[0m                      \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                      \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=5,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vocabulary Table:\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.11 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model:\n",
    "_Parameters of the training:_\n",
    "* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n",
    "* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 5.6 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(tokenized_paras, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('མཐུ་', 0.608696699142456),\n",
       " ('ནུས་སྟོབས་', 0.45071011781692505),\n",
       " ('ཤུགས་', 0.4431358575820923),\n",
       " ('གོམས་པ་', 0.4223802387714386),\n",
       " ('སོར་ཆུད་པ་', 0.42098259925842285),\n",
       " ('ཡོན་ཏན་', 0.4176880419254303),\n",
       " ('སྐྱེས་ཐོབ་', 0.4152085483074188),\n",
       " ('ལྡན་པ་', 0.41036728024482727),\n",
       " ('མཐུ་ཆེན་པོ་', 0.4098166823387146),\n",
       " ('བརྩོན་འགྲུས་', 0.4016813039779663)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"སྟོབས་\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('མཛད་', 0.7612118721008301),\n",
       " ('ཕྲིན་ལས་', 0.5403375029563904),\n",
       " ('མཛད་པ་པོ་', 0.49853041768074036),\n",
       " ('སྟོན་པ་', 0.4814871549606323),\n",
       " ('འགྲེལ་པ་', 0.47240862250328064),\n",
       " ('སྒྲུབ་ཐབས་', 0.4716480076313019),\n",
       " ('གྲགས་པ་', 0.46867769956588745),\n",
       " ('གདུལ་བྱ་', 0.4681282937526703),\n",
       " ('བསྟན་པ་', 0.4617062211036682),\n",
       " ('འཕྲིན་ལས་', 0.4564966857433319)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"མཛད་པ་\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('བྱིན་རླབས་', 0.6062471866607666),\n",
       " ('མོས་གུས་', 0.564081072807312),\n",
       " ('མཚན་ལྡན་', 0.5319912433624268),\n",
       " ('རྗེ་', 0.5182524919509888),\n",
       " ('བཀའ་དྲིན་', 0.5181214809417725),\n",
       " ('བརྒྱུད་པ་', 0.5165523886680603),\n",
       " ('ཡི་དམ་', 0.5100193023681641),\n",
       " ('དད་གུས་', 0.5081140995025635),\n",
       " ('དྲིན་ཅན་', 0.4949682950973511),\n",
       " ('སྐྱབས་གནས་', 0.48522794246673584)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"བླ་མ་\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ལ་རྩ་བ་', 0.504997730255127),\n",
       " ('མདོར་ན་', 0.40671637654304504),\n",
       " ('ཐམས་ཅད་', 0.37312084436416626),\n",
       " ('བསྐྱེད་པ་', 0.36299213767051697),\n",
       " ('རྟེན་གཞི་', 0.3564617335796356),\n",
       " ('རྒྱུད་', 0.35559332370758057),\n",
       " ('སྡོམ་པ་', 0.35458317399024963),\n",
       " ('གཙོ་བོ་', 0.35075682401657104),\n",
       " ('གསུམ་པོ་', 0.3506447374820709),\n",
       " ('སྤྱིར་', 0.3458601236343384)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"རྩ་བ་\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('དེ་ཉིད་', 0.784591794013977),\n",
       " ('རང་བཞིན་', 0.6753857731819153),\n",
       " ('ངོ་བོ་', 0.6689523458480835),\n",
       " ('བདག་ཉིད་', 0.6479246616363525),\n",
       " ('ནི་', 0.636000394821167),\n",
       " ('ཕྱི་', 0.6137641668319702),\n",
       " ('དེ་བཞིན་ཉིད་', 0.5992171764373779),\n",
       " ('ནོ་', 0.5715460181236267),\n",
       " ('ཞེ་', 0.5701719522476196),\n",
       " ('གི་', 0.5472181439399719)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"ཉིད་\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.save_word2vec_format(\"./bo_word2vec_lammatized\",\n",
    "                              \"./vocab\",\n",
    "                               binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  __output__.json  bo_word2vec_lammatized  vocab\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load_word2vec_format('bo_word2vec_lammatized', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('དེ་ཉིད་', 0.784591794013977),\n",
       " ('རང་བཞིན་', 0.6753857731819153),\n",
       " ('ངོ་བོ་', 0.6689522862434387),\n",
       " ('བདག་ཉིད་', 0.6479246616363525),\n",
       " ('ནི་', 0.636000394821167),\n",
       " ('ཕྱི་', 0.613764226436615),\n",
       " ('དེ་བཞིན་ཉིད་', 0.5992171764373779),\n",
       " ('ནོ་', 0.5715460181236267),\n",
       " ('ཞེ་', 0.5701720118522644),\n",
       " ('གི་', 0.5472180843353271)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text.wv.most_similar(positive=[\"ཉིད་\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
